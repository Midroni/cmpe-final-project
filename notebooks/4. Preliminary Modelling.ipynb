{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Preliminary Modelling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Train-Test Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_df = pd.read_pickle('../data/features_df.pkl')\n",
    "meta_df = pd.read_csv('../data/speechdetails.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removes all the text that was in before\n",
    "feat_df = feat_df.select_dtypes(exclude=['object'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = feat_df.values\n",
    "y = meta_df['IC'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, \n",
    "                                                    test_size=0.2,\n",
    "                                                    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in the dataframes\n",
    "feat_df = pd.read_pickle('../data/features_df.pkl')\n",
    "meta_df = pd.read_csv('../data/speechdetails.csv')\n",
    "\n",
    "#Removes all the text that was in before\n",
    "feat_df = feat_df.select_dtypes(exclude=['object'])\n",
    "\n",
    "#Isolate predictive (X) and target (y) variables\n",
    "X = feat_df.values\n",
    "y = meta_df['IC'].values\n",
    "\n",
    "#Implement train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, \n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_reg = xgb.XGBRegressor(objective ='reg:linear', colsample_bytree = 0.3, learning_rate = 0.1,\n",
    "                max_depth = 5, alpha = 10, n_estimators = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(alpha=10, base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=0.3, gamma=0, importance_type='gain',\n",
       "       learning_rate=0.1, max_delta_step=0, max_depth=5,\n",
       "       min_child_weight=1, missing=None, n_estimators=10, n_jobs=1,\n",
       "       nthread=None, objective='reg:linear', random_state=0, reg_alpha=0,\n",
       "       reg_lambda=1, scale_pos_weight=1, seed=None, silent=True,\n",
       "       subsample=1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xg_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = xg_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 0.4548896045049032\n",
      "Mean Squared Error: 0.2840524020486156\n",
      "Root Mean Squared Error: 0.532965666857271\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try with K-fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dmatrix = xgb.DMatrix(data=X,label=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"objective\":\"reg:linear\",'colsample_bytree': 0.3,'learning_rate': 0.1,\n",
    "                'max_depth': 5, 'alpha': 10}\n",
    "\n",
    "cv_results = xgb.cv(dtrain=data_dmatrix, params=params, nfold=3,\n",
    "                    num_boost_round=50,early_stopping_rounds=10,metrics=\"rmse\", as_pandas=True, seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train-rmse-mean</th>\n",
       "      <th>train-rmse-std</th>\n",
       "      <th>test-rmse-mean</th>\n",
       "      <th>test-rmse-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.222118</td>\n",
       "      <td>0.039675</td>\n",
       "      <td>1.220663</td>\n",
       "      <td>0.087276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.124399</td>\n",
       "      <td>0.036571</td>\n",
       "      <td>1.124137</td>\n",
       "      <td>0.089193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.037226</td>\n",
       "      <td>0.033902</td>\n",
       "      <td>1.037717</td>\n",
       "      <td>0.093476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.959700</td>\n",
       "      <td>0.031666</td>\n",
       "      <td>0.960710</td>\n",
       "      <td>0.097018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.890930</td>\n",
       "      <td>0.029815</td>\n",
       "      <td>0.892631</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.829778</td>\n",
       "      <td>0.028143</td>\n",
       "      <td>0.832865</td>\n",
       "      <td>0.100951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.775398</td>\n",
       "      <td>0.027006</td>\n",
       "      <td>0.780085</td>\n",
       "      <td>0.102865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.727083</td>\n",
       "      <td>0.025794</td>\n",
       "      <td>0.733415</td>\n",
       "      <td>0.104681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.685052</td>\n",
       "      <td>0.025115</td>\n",
       "      <td>0.692368</td>\n",
       "      <td>0.105850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.647245</td>\n",
       "      <td>0.024850</td>\n",
       "      <td>0.655065</td>\n",
       "      <td>0.105256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.613789</td>\n",
       "      <td>0.024791</td>\n",
       "      <td>0.622806</td>\n",
       "      <td>0.103718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.585227</td>\n",
       "      <td>0.024402</td>\n",
       "      <td>0.595627</td>\n",
       "      <td>0.101948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.559477</td>\n",
       "      <td>0.024079</td>\n",
       "      <td>0.571584</td>\n",
       "      <td>0.099944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.537204</td>\n",
       "      <td>0.023444</td>\n",
       "      <td>0.549808</td>\n",
       "      <td>0.099581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.517683</td>\n",
       "      <td>0.023315</td>\n",
       "      <td>0.532311</td>\n",
       "      <td>0.098438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.500941</td>\n",
       "      <td>0.023614</td>\n",
       "      <td>0.517169</td>\n",
       "      <td>0.097381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.485401</td>\n",
       "      <td>0.023561</td>\n",
       "      <td>0.503486</td>\n",
       "      <td>0.096634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.471570</td>\n",
       "      <td>0.022867</td>\n",
       "      <td>0.492134</td>\n",
       "      <td>0.095815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.459720</td>\n",
       "      <td>0.022554</td>\n",
       "      <td>0.481855</td>\n",
       "      <td>0.093937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.449000</td>\n",
       "      <td>0.023243</td>\n",
       "      <td>0.473303</td>\n",
       "      <td>0.092490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.440403</td>\n",
       "      <td>0.023258</td>\n",
       "      <td>0.466044</td>\n",
       "      <td>0.090791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.431777</td>\n",
       "      <td>0.023196</td>\n",
       "      <td>0.459333</td>\n",
       "      <td>0.089490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.424337</td>\n",
       "      <td>0.023077</td>\n",
       "      <td>0.454039</td>\n",
       "      <td>0.088454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.418072</td>\n",
       "      <td>0.023150</td>\n",
       "      <td>0.449022</td>\n",
       "      <td>0.087383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.412273</td>\n",
       "      <td>0.023075</td>\n",
       "      <td>0.444953</td>\n",
       "      <td>0.085674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.407100</td>\n",
       "      <td>0.023597</td>\n",
       "      <td>0.441671</td>\n",
       "      <td>0.084516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.402733</td>\n",
       "      <td>0.023353</td>\n",
       "      <td>0.438747</td>\n",
       "      <td>0.083262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.398828</td>\n",
       "      <td>0.022861</td>\n",
       "      <td>0.435676</td>\n",
       "      <td>0.082325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.394857</td>\n",
       "      <td>0.022865</td>\n",
       "      <td>0.433089</td>\n",
       "      <td>0.081847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.391200</td>\n",
       "      <td>0.023033</td>\n",
       "      <td>0.431179</td>\n",
       "      <td>0.080634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.388076</td>\n",
       "      <td>0.022866</td>\n",
       "      <td>0.429569</td>\n",
       "      <td>0.079902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.385383</td>\n",
       "      <td>0.023057</td>\n",
       "      <td>0.427902</td>\n",
       "      <td>0.078825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.383133</td>\n",
       "      <td>0.023060</td>\n",
       "      <td>0.426803</td>\n",
       "      <td>0.078292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.380382</td>\n",
       "      <td>0.022325</td>\n",
       "      <td>0.425342</td>\n",
       "      <td>0.077801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.378384</td>\n",
       "      <td>0.022269</td>\n",
       "      <td>0.424274</td>\n",
       "      <td>0.077012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.376384</td>\n",
       "      <td>0.022463</td>\n",
       "      <td>0.423468</td>\n",
       "      <td>0.076388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.374815</td>\n",
       "      <td>0.022510</td>\n",
       "      <td>0.422588</td>\n",
       "      <td>0.076076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.372938</td>\n",
       "      <td>0.021957</td>\n",
       "      <td>0.421757</td>\n",
       "      <td>0.075608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.371732</td>\n",
       "      <td>0.021893</td>\n",
       "      <td>0.421052</td>\n",
       "      <td>0.075237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.370617</td>\n",
       "      <td>0.022211</td>\n",
       "      <td>0.420474</td>\n",
       "      <td>0.074649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.369597</td>\n",
       "      <td>0.022382</td>\n",
       "      <td>0.419748</td>\n",
       "      <td>0.074142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.368517</td>\n",
       "      <td>0.022181</td>\n",
       "      <td>0.419276</td>\n",
       "      <td>0.073866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.367825</td>\n",
       "      <td>0.022116</td>\n",
       "      <td>0.418795</td>\n",
       "      <td>0.073606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.367116</td>\n",
       "      <td>0.022309</td>\n",
       "      <td>0.418337</td>\n",
       "      <td>0.073299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.366182</td>\n",
       "      <td>0.022109</td>\n",
       "      <td>0.418056</td>\n",
       "      <td>0.073004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.365676</td>\n",
       "      <td>0.022197</td>\n",
       "      <td>0.417726</td>\n",
       "      <td>0.072642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.364938</td>\n",
       "      <td>0.022002</td>\n",
       "      <td>0.417611</td>\n",
       "      <td>0.072385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.364459</td>\n",
       "      <td>0.021953</td>\n",
       "      <td>0.417546</td>\n",
       "      <td>0.072134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.363531</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.417404</td>\n",
       "      <td>0.071855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.362991</td>\n",
       "      <td>0.021857</td>\n",
       "      <td>0.417169</td>\n",
       "      <td>0.071612</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    train-rmse-mean  train-rmse-std  test-rmse-mean  test-rmse-std\n",
       "0          1.222118        0.039675        1.220663       0.087276\n",
       "1          1.124399        0.036571        1.124137       0.089193\n",
       "2          1.037226        0.033902        1.037717       0.093476\n",
       "3          0.959700        0.031666        0.960710       0.097018\n",
       "4          0.890930        0.029815        0.892631       0.100000\n",
       "5          0.829778        0.028143        0.832865       0.100951\n",
       "6          0.775398        0.027006        0.780085       0.102865\n",
       "7          0.727083        0.025794        0.733415       0.104681\n",
       "8          0.685052        0.025115        0.692368       0.105850\n",
       "9          0.647245        0.024850        0.655065       0.105256\n",
       "10         0.613789        0.024791        0.622806       0.103718\n",
       "11         0.585227        0.024402        0.595627       0.101948\n",
       "12         0.559477        0.024079        0.571584       0.099944\n",
       "13         0.537204        0.023444        0.549808       0.099581\n",
       "14         0.517683        0.023315        0.532311       0.098438\n",
       "15         0.500941        0.023614        0.517169       0.097381\n",
       "16         0.485401        0.023561        0.503486       0.096634\n",
       "17         0.471570        0.022867        0.492134       0.095815\n",
       "18         0.459720        0.022554        0.481855       0.093937\n",
       "19         0.449000        0.023243        0.473303       0.092490\n",
       "20         0.440403        0.023258        0.466044       0.090791\n",
       "21         0.431777        0.023196        0.459333       0.089490\n",
       "22         0.424337        0.023077        0.454039       0.088454\n",
       "23         0.418072        0.023150        0.449022       0.087383\n",
       "24         0.412273        0.023075        0.444953       0.085674\n",
       "25         0.407100        0.023597        0.441671       0.084516\n",
       "26         0.402733        0.023353        0.438747       0.083262\n",
       "27         0.398828        0.022861        0.435676       0.082325\n",
       "28         0.394857        0.022865        0.433089       0.081847\n",
       "29         0.391200        0.023033        0.431179       0.080634\n",
       "30         0.388076        0.022866        0.429569       0.079902\n",
       "31         0.385383        0.023057        0.427902       0.078825\n",
       "32         0.383133        0.023060        0.426803       0.078292\n",
       "33         0.380382        0.022325        0.425342       0.077801\n",
       "34         0.378384        0.022269        0.424274       0.077012\n",
       "35         0.376384        0.022463        0.423468       0.076388\n",
       "36         0.374815        0.022510        0.422588       0.076076\n",
       "37         0.372938        0.021957        0.421757       0.075608\n",
       "38         0.371732        0.021893        0.421052       0.075237\n",
       "39         0.370617        0.022211        0.420474       0.074649\n",
       "40         0.369597        0.022382        0.419748       0.074142\n",
       "41         0.368517        0.022181        0.419276       0.073866\n",
       "42         0.367825        0.022116        0.418795       0.073606\n",
       "43         0.367116        0.022309        0.418337       0.073299\n",
       "44         0.366182        0.022109        0.418056       0.073004\n",
       "45         0.365676        0.022197        0.417726       0.072642\n",
       "46         0.364938        0.022002        0.417611       0.072385\n",
       "47         0.364459        0.021953        0.417546       0.072134\n",
       "48         0.363531        0.021739        0.417404       0.071855\n",
       "49         0.362991        0.021857        0.417169       0.071612"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49    0.417169\n",
      "Name: test-rmse-mean, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print((cv_results[\"test-rmse-mean\"]).tail(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Boosting Trees and Feature Importance\n",
    "We can visualize individual trees from the fully boosted model that XGBoost creates using the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, importance_type='gain',\n",
       "       learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
       "       min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
       "       nthread=None, objective='reg:linear', random_state=0, reg_alpha=0,\n",
       "       reg_lambda=1, scale_pos_weight=1, seed=None, silent=True,\n",
       "       subsample=1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = xgb.XGBRegressor()\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0187495  0.05512465 0.04033196 0.03925942 0.         0.\n",
      " 0.04000152 0.05356915 0.10524248 0.05023061 0.03193491 0.02034778\n",
      " 0.06616806 0.03187322 0.03114282 0.03238795 0.         0.\n",
      " 0.         0.         0.         0.         0.027601   0.11083944\n",
      " 0.04850224 0.07841585 0.04581972 0.07245771]\n"
     ]
    }
   ],
   "source": [
    "print(model.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "regressor = RandomForestRegressor(n_estimators=20, random_state=0)\n",
    "regressor.fit(X_train, y_train)\n",
    "y_pred = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 0.30050277799999997\n",
      "Mean Squared Error: 0.12828849560959255\n",
      "Root Mean Squared Error: 0.3581738343452695\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Curve for Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[54778575.118644066,\n",
       " 44252988.779661015,\n",
       " 35667868.81355932,\n",
       " 27857307.627118643,\n",
       " 21748981.694915254,\n",
       " 16684007.661016949,\n",
       " 12526525.949152542,\n",
       " 9159840.847457627,\n",
       " 6626329.824152542,\n",
       " 4503556.813559322,\n",
       " 2972132.686440678,\n",
       " 1909849.309322034,\n",
       " 1196887.3559322034,\n",
       " 668439.1663135593,\n",
       " 349411.0995762712,\n",
       " 164143.29157838982,\n",
       " 67845.00986493644,\n",
       " 19401.686093087923,\n",
       " 5181.098448672537,\n",
       " 3668.745506157309,\n",
       " 7456.424953654661,\n",
       " 10905.938228283898,\n",
       " 12425.25627317267,\n",
       " 12066.577926377118,\n",
       " 10475.606660487289,\n",
       " 8446.452338784427,\n",
       " 6495.164426641949,\n",
       " 4947.04001423464,\n",
       " 3966.537614208157,\n",
       " 3315.4402765823625,\n",
       " 3099.3629998675847,\n",
       " 2957.2897576800847,\n",
       " 2936.9808825476694,\n",
       " 2967.433461334746,\n",
       " 2972.8956133309057,\n",
       " 2974.2733878442796,\n",
       " 2968.0256678694386,\n",
       " 2956.604305978549,\n",
       " 2946.327351198358,\n",
       " 2933.4033285884534,\n",
       " 2933.1307228217693,\n",
       " 2927.221108646716,\n",
       " 2926.891531216896,\n",
       " 2924.348777641684,\n",
       " 2925.479939088983,\n",
       " 2925.7107968087926,\n",
       " 2925.2032532772773,\n",
       " 2924.1513713254767,\n",
       " 2924.403576867055,\n",
       " 2922.100180415784,\n",
       " 2919.5367866128177,\n",
       " 2917.841267213983,\n",
       " 2916.809773073358,\n",
       " 2916.7017677436443,\n",
       " 2914.4097548662608,\n",
       " 2915.0591151350636,\n",
       " 2915.2972391419494,\n",
       " 2912.8686378608318,\n",
       " 2911.4070444915255,\n",
       " 2913.6356014962926,\n",
       " 2909.4253012447034,\n",
       " 2910.176368842691,\n",
       " 2913.3366492319915,\n",
       " 2907.623911712129,\n",
       " 2904.6826833951272,\n",
       " 2904.901296841896,\n",
       " 2903.171804654396,\n",
       " 2903.338825807733,\n",
       " 2906.545298430879,\n",
       " 2899.1236758474574,\n",
       " 2904.318980071504,\n",
       " 2896.7380040055614,\n",
       " 2900.733303264036,\n",
       " 2895.022556110964,\n",
       " 2892.9094196901483,\n",
       " 2895.4504601430085,\n",
       " 2891.0649331302966,\n",
       " 2890.3052957825744,\n",
       " 2888.441923497087,\n",
       " 2890.8206600900426,\n",
       " 2888.5954962261653,\n",
       " 2890.6817647643006,\n",
       " 2887.5025820974574,\n",
       " 2888.37790485964,\n",
       " 2885.100991459216,\n",
       " 2880.9495290982522,\n",
       " 2880.667927370233,\n",
       " 2877.4114390227755,\n",
       " 2874.7575600834216,\n",
       " 2876.3175193657307,\n",
       " 2878.231577727754,\n",
       " 2872.256662142479,\n",
       " 2872.872244107521,\n",
       " 2869.9652285818324,\n",
       " 2871.5891568458687,\n",
       " 2876.5531192068324,\n",
       " 2871.6969842425847,\n",
       " 2867.489204018803,\n",
       " 2862.5594482421875,\n",
       " 2865.90821553893,\n",
       " 2860.184516518803,\n",
       " 2860.8724799721927,\n",
       " 2858.035210043697,\n",
       " 2856.301530223782,\n",
       " 2862.8868097854875,\n",
       " 2855.82225321107,\n",
       " 2858.530331369174,\n",
       " 2854.0599799721927,\n",
       " 2850.41063211732,\n",
       " 2849.7594883805614,\n",
       " 2850.676199185646,\n",
       " 2846.096936242055,\n",
       " 2844.0413963188557,\n",
       " 2841.663133524232,\n",
       " 2842.5225271451272,\n",
       " 2842.89259053893,\n",
       " 2837.9969461731994,\n",
       " 2837.1630859375,\n",
       " 2834.4566629700744,\n",
       " 2834.2769175384005,\n",
       " 2836.283745199947,\n",
       " 2829.9900109242585,\n",
       " 2827.571615962659,\n",
       " 2827.418891518803,\n",
       " 2827.57615946107,\n",
       " 2825.4654354806676,\n",
       " 2825.253989009534,\n",
       " 2821.4355675648835,\n",
       " 2832.6900696835273,\n",
       " 2817.7743809586864,\n",
       " 2816.7789658368642,\n",
       " 2815.8832800913665,\n",
       " 2812.862416412871,\n",
       " 2810.4845156912074,\n",
       " 2810.9278378244176,\n",
       " 2810.9664616988875,\n",
       " 2808.3749503442796,\n",
       " 2809.2256893869176,\n",
       " 2803.3547032243114,\n",
       " 2801.1389077396716,\n",
       " 2798.829387082892,\n",
       " 2803.2093816207625,\n",
       " 2797.8240201271187,\n",
       " 2795.5670476363875,\n",
       " 2793.6638638771187,\n",
       " 2796.0808767545022,\n",
       " 2795.5929803363347,\n",
       " 2792.7794913599046,\n",
       " 2788.036614886785,\n",
       " 2783.8990292306676,\n",
       " 2793.3722689353813,\n",
       " 2780.834605071504,\n",
       " 2781.2209141618114,\n",
       " 2781.9483663268006,\n",
       " 2777.4311192399364,\n",
       " 2774.400895458157,\n",
       " 2775.0660545219807,\n",
       " 2771.8136420815677,\n",
       " 2772.5188319319386,\n",
       " 2768.067680746822,\n",
       " 2764.2325253244176,\n",
       " 2765.64451883607,\n",
       " 2762.1100163863875,\n",
       " 2760.5604972192796,\n",
       " 2758.3770027807204,\n",
       " 2760.020590572034,\n",
       " 2753.8865904727227,\n",
       " 2751.5130553164727,\n",
       " 2751.2973177635063,\n",
       " 2759.395114704714,\n",
       " 2752.5941472457625,\n",
       " 2745.6635080111228,\n",
       " 2743.9746714446505,\n",
       " 2740.8155331369176,\n",
       " 2743.9698920815677,\n",
       " 2736.7224990068858,\n",
       " 2736.504771087129,\n",
       " 2735.3884360103284,\n",
       " 2731.817651780985,\n",
       " 2735.9570643538136,\n",
       " 2745.5763104972193,\n",
       " 2738.3999768273306,\n",
       " 2727.548012943591,\n",
       " 2722.2658732786017,\n",
       " 2720.460192664195,\n",
       " 2730.908203125,\n",
       " 2718.9443028336864,\n",
       " 2713.7873990333687,\n",
       " 2712.9565802105403,\n",
       " 2715.9335027145125,\n",
       " 2711.438608977754,\n",
       " 2707.381736626059,\n",
       " 2710.509391138109,\n",
       " 2703.8245663400426,\n",
       " 2702.186453091896,\n",
       " 2705.2426012976694,\n",
       " 2698.958384368379,\n",
       " 2701.193702827066,\n",
       " 2703.3316836599574,\n",
       " 2696.681495795816]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = [47249372.881355934, 38306795.59322034, 30496030.101694915, 23914019.050847456, 18367448.06779661, 13821742.508474575, 10158476.677966101, 7278298.016949153, 5052215.610169492, 3419284.194915254, 2233431.1016949154, 1389939.5, 807155.3262711865, 436049.52966101695, 212049.6435381356, 88569.68855932204, 29089.809586864405, 6906.328853283899, 3025.3364878509005, 6082.789608712924, 9978.95224774894, 12225.243991657839, 12408.116227489407, 11126.867915783898, 9153.776698225636, 7117.212360963983, 5422.773627846928, 4214.0295699814615, 3469.132022146451, 3099.2443516618114, 2955.493681309587, 2935.1299903998943, 2952.4478945974574, 2972.384749073093, 2972.9484242584745, 2971.580227092161, 2961.7901135460806, 2950.2542869438557, 2939.7535255561443, 2934.0513564287608, 2928.2417654263772, 2926.0384211136125, 2925.271095405191, 2924.6971414856994, 2924.844267247087, 2924.3919988082625, 2923.322307004767, 2922.009223550053, 2921.1656432070977, 2919.449545650159, 2918.35692117982, 2917.122463420286, 2915.9873833090573, 2914.9199715307204, 2914.1341904462392, 2913.5039145259534, 2912.1346456236756, 2911.264892578125, 2910.3287332825744, 2908.845558295816, 2907.8761172537074, 2906.9536339711335, 2905.7791272179556, 2904.850424556409, 2903.6636776681676, 2902.4249329647773, 2901.3584315413136, 2900.3485666048728, 2899.1073018736756, 2897.9701817399364, 2896.853354243909, 2895.7259873212392, 2895.1455285023835, 2893.8194766287074, 2892.2333653336864, 2891.1339587195444, 2889.922143968485, 2888.6610955707097, 2887.4551650225108, 2886.1924283302437, 2885.7139747748943, 2884.0647593352755, 2884.0575840836864, 2882.8662440413136, 2881.9705955376057, 2879.6670997748943, 2878.0169698424256, 2876.0599965240995, 2875.013485666049, 2874.6502995895125, 2872.7287680415784, 2871.9047065346927, 2870.430448887712, 2869.4886412539727, 2867.7016560182733, 2868.1429298530193, 2865.1268372616523, 2863.311391022246, 2861.6662556276483, 2860.353557004767, 2859.1530099642478, 2857.852108712924, 2856.4526822364937, 2854.7940818657307, 2853.2739175052966, 2852.1541437698625, 2850.4583388506358, 2850.2251473119704, 2848.92722126589, 2847.28126241393, 2846.395425052966, 2844.1924490201272, 2842.4428793697034, 2840.470417604608, 2838.7862197100108, 2838.107310149629, 2836.4529305150954, 2835.1059611692267, 2833.1013845670022, 2831.3252615201272, 2829.886147709216, 2828.2281307931676, 2826.61036397643, 2825.3891518802966, 2824.457738844015, 2823.3062806210273, 2821.6281779661017, 2819.391398801642, 2817.8105799788136, 2816.414811473782, 2814.453803628178, 2812.5274306475108, 2810.8874718617585, 2809.2938170352227, 2807.9369496490995, 2807.5009848384534, 2805.895780918962, 2802.7984308792375, 2801.1825385659427, 2799.378028998941, 2798.059897212659, 2796.82423116393, 2795.104521153337, 2792.85939982786, 2790.9799887447034, 2789.649145094015, 2787.55809305482, 2786.0087021649892, 2784.315818657309, 2782.4409965903073, 2781.2707850569386, 2778.9955599510063, 2777.305001986229, 2775.49022196107, 2773.68361857786, 2771.938865532309, 2770.1473574880824, 2768.522854045286, 2766.5631579382944, 2764.7797768802966, 2763.0565951072563, 2761.188691737288, 2759.4117617849574, 2757.8123013771187, 2755.83102985964, 2754.2630304886125, 2752.062483448093, 2750.3391982256358, 2748.6478995630296, 2748.6040949417375, 2745.174589512712, 2743.1366980601165, 2740.9369786149364, 2739.0353672868114, 2737.6360897775426, 2735.3613777807204, 2733.36618610964, 2732.0523329912608, 2730.0429232322563, 2728.3402409957625, 2726.392809851695, 2726.003078654661, 2722.14210225768, 2720.0756049721927, 2718.100192829714, 2717.6180978548728, 2714.3309181342693, 2712.2337708554023, 2710.3550839181676, 2708.2619297868114, 2707.2809313757944, 2704.93895242982, 2703.7861410884534, 2700.4201991194386, 2698.231838420286, 2698.6197571835273, 2697.2058560646187, 2693.0906796212926, 2691.7652443061443, 2688.240681276483]\n",
    "[54778575.118644066, 44252988.779661015, 35667868.81355932, 27857307.627118643, 21748981.694915254, 16684007.661016949, 12526525.949152542, 9159840.847457627, 6626329.824152542, 4503556.813559322, 2972132.686440678, 1909849.309322034, 1196887.3559322034, 668439.1663135593, 349411.0995762712, 164143.29157838982, 67845.00986493644, 19401.686093087923, 5181.098448672537, 3668.745506157309, 7456.424953654661, 10905.938228283898, 12425.25627317267, 12066.577926377118, 10475.606660487289, 8446.452338784427, 6495.164426641949, 4947.04001423464, 3966.537614208157, 3315.4402765823625, 3099.3629998675847, 2957.2897576800847, 2936.9808825476694, 2967.433461334746, 2972.8956133309057, 2974.2733878442796, 2968.0256678694386, 2956.604305978549, 2946.327351198358, 2933.4033285884534, 2933.1307228217693, 2927.221108646716, 2926.891531216896, 2924.348777641684, 2925.479939088983, 2925.7107968087926, 2925.2032532772773, 2924.1513713254767, 2924.403576867055, 2922.100180415784, 2919.5367866128177, 2917.841267213983, 2916.809773073358, 2916.7017677436443, 2914.4097548662608, 2915.0591151350636, 2915.2972391419494, 2912.8686378608318, 2911.4070444915255, 2913.6356014962926, 2909.4253012447034, 2910.176368842691, 2913.3366492319915, 2907.623911712129, 2904.6826833951272, 2904.901296841896, 2903.171804654396, 2903.338825807733, 2906.545298430879, 2899.1236758474574, 2904.318980071504, 2896.7380040055614, 2900.733303264036, 2895.022556110964, 2892.9094196901483, 2895.4504601430085, 2891.0649331302966, 2890.3052957825744, 2888.441923497087, 2890.8206600900426, 2888.5954962261653, 2890.6817647643006, 2887.5025820974574, 2888.37790485964, 2885.100991459216, 2880.9495290982522, 2880.667927370233, 2877.4114390227755, 2874.7575600834216, 2876.3175193657307, 2878.231577727754, 2872.256662142479, 2872.872244107521, 2869.9652285818324, 2871.5891568458687, 2876.5531192068324, 2871.6969842425847, 2867.489204018803, 2862.5594482421875, 2865.90821553893, 2860.184516518803, 2860.8724799721927, 2858.035210043697, 2856.301530223782, 2862.8868097854875, 2855.82225321107, 2858.530331369174, 2854.0599799721927, 2850.41063211732, 2849.7594883805614, 2850.676199185646, 2846.096936242055, 2844.0413963188557, 2841.663133524232, 2842.5225271451272, 2842.89259053893, 2837.9969461731994, 2837.1630859375, 2834.4566629700744, 2834.2769175384005, 2836.283745199947, 2829.9900109242585, 2827.571615962659, 2827.418891518803, 2827.57615946107, 2825.4654354806676, 2825.253989009534, 2821.4355675648835, 2832.6900696835273, 2817.7743809586864, 2816.7789658368642, 2815.8832800913665, 2812.862416412871, 2810.4845156912074, 2810.9278378244176, 2810.9664616988875, 2808.3749503442796, 2809.2256893869176, 2803.3547032243114, 2801.1389077396716, 2798.829387082892, 2803.2093816207625, 2797.8240201271187, 2795.5670476363875, 2793.6638638771187, 2796.0808767545022, 2795.5929803363347, 2792.7794913599046, 2788.036614886785, 2783.8990292306676, 2793.3722689353813, 2780.834605071504, 2781.2209141618114, 2781.9483663268006, 2777.4311192399364, 2774.400895458157, 2775.0660545219807, 2771.8136420815677, 2772.5188319319386, 2768.067680746822, 2764.2325253244176, 2765.64451883607, 2762.1100163863875, 2760.5604972192796, 2758.3770027807204, 2760.020590572034, 2753.8865904727227, 2751.5130553164727, 2751.2973177635063, 2759.395114704714, 2752.5941472457625, 2745.6635080111228, 2743.9746714446505, 2740.8155331369176, 2743.9698920815677, 2736.7224990068858, 2736.504771087129, 2735.3884360103284, 2731.817651780985, 2735.9570643538136, 2745.5763104972193, 2738.3999768273306, 2727.548012943591, 2722.2658732786017, 2720.460192664195, 2730.908203125, 2718.9443028336864, 2713.7873990333687, 2712.9565802105403, 2715.9335027145125, 2711.438608977754, 2707.381736626059, 2710.509391138109, 2703.8245663400426, 2702.186453091896, 2705.2426012976694, 2698.958384368379, 2701.193702827066, 2703.3316836599574, 2696.681495795816]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
